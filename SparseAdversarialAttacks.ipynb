{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MariaBulychev/Thesis_Sparse_Adversarial_Attacks/blob/main/SparseAdversarialAttacks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAdB5PbprwPP"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvN3nbAzru7v"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torch\n",
        "import random\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter # TensorBoard support\n",
        "import torchvision.datasets as datasets\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable, grad\n",
        "import numpy as np\n",
        "import argparse\n",
        "import copy\n",
        "plt.figure(figsize = (3,3)) #define the image size\n",
        "import scipy.io\n",
        "import time\n",
        "\n",
        "!pip install wandb\n",
        "import wandb\n",
        "\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa_zQf1Pr9G0"
      },
      "source": [
        "# Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBAu-8-2sBCN"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    #config  = defaults,     \n",
        "    project = 'SparseAdv',   \n",
        "    entity  = 'MariaBulychev',       \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XQgr2lPaqdvq"
      },
      "outputs": [],
      "source": [
        "device='cuda'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vSiLzZ9sEHT"
      },
      "source": [
        "# Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpzhTNMXsHaT"
      },
      "outputs": [],
      "source": [
        "#@title FashionCNN { form-width: \"200px\" }\n",
        "#@markdown Do not set shuffle if want to reproduce the results\n",
        "shuffle = False #@param {type: \"boolean\"}\n",
        "#@markdown Batch size\n",
        "b_size =  32#@param {type: \"integer\"}\n",
        "test_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, train=False, transform=\n",
        "                                               transforms.Compose([transforms.ToTensor()])) \n",
        "test_loader = torch.utils.data.DataLoader(test_set,\n",
        "                                          batch_size=b_size, shuffle=shuffle, drop_last=True)\n",
        "pretrained_model = True #@param {type: \"boolean\"}\n",
        "save_model = False #@param {type: \"boolean\"}\n",
        "model_label = \"v1\" #@param {type: \"string\"}\n",
        "# Build a CNN model\n",
        "class FashionCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionCNN, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc1 = nn.Linear(in_features=64*6*6, out_features=600)\n",
        "        self.drop = nn.Dropout2d(0.25)\n",
        "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
        "        self.fc3 = nn.Linear(in_features=120, out_features=10)\n",
        "    def forward(self, x):\n",
        "        if len(x.shape) == 4 and x.shape[3]==1:\n",
        "             x = x.permute(0,3,2,1)\n",
        "        #if len(x.shape) == 3 and x.shape[2]==1:\n",
        "             #x = x.permute(0,2,1)\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        #out = out.view(out.size(0), -1)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.drop(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "if not pretrained_model:\n",
        "    train_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, transform=\n",
        "                                                    transforms.Compose([transforms.ToTensor()]))\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, \n",
        "                                              batch_size=b_size, drop_last=True)\n",
        "    # Make a model of CNN class\n",
        "    model = FashionCNN()\n",
        "    model.to(device)\n",
        "    error = nn.CrossEntropyLoss()\n",
        "    learning_rate = 0.001\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    #print(model)\n",
        "    num_epochs = 5\n",
        "    count = 0\n",
        "    # Lists for visualization of loss and accuracy \n",
        "    loss_list = []\n",
        "    iteration_list = []\n",
        "    accuracy_list = []\n",
        "    # Lists for knowing class-wise accuracy\n",
        "    predictions_list = []\n",
        "    labels_list = []\n",
        "    for epoch in range(num_epochs):\n",
        "        for images, labels in train_loader:\n",
        "            # Transfering images and labels to GPU if available\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            train = Variable(images.view(b_size, 1, 28, 28))\n",
        "            labels = Variable(labels)\n",
        "            # Forward pass \n",
        "            outputs = model(train)\n",
        "            loss = error(outputs, labels)\n",
        "            # Initializing a gradient as 0 so there is no mixing of gradient among the batches\n",
        "            optimizer.zero_grad()\n",
        "            # Propagating the error backward\n",
        "            loss.backward()\n",
        "            # Optimizing the parameters\n",
        "            optimizer.step()\n",
        "            count += 1\n",
        "        # Testing the model\n",
        "            if not (count % 50):    # It's same as \"if count % 50 == 0\"\n",
        "                total = 0\n",
        "                correct = 0\n",
        "                for images, labels in test_loader:\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "                    labels_list.append(labels)\n",
        "                    test = Variable(images.view(b_size, 1, 28, 28))\n",
        "                    outputs = model(test)\n",
        "                    predictions = torch.max(outputs, 1)[1].to(device)\n",
        "                    predictions_list.append(predictions)\n",
        "                    correct += (predictions == labels).sum()\n",
        "                    total += len(labels)\n",
        "                accuracy = correct * 100 / total\n",
        "                loss_list.append(loss.data)\n",
        "                iteration_list.append(count)\n",
        "                accuracy_list.append(accuracy)\n",
        "            if not (count % 500):\n",
        "                print(\"Iteration: {}, Loss: {}, Accuracy: {}%\".format(count, loss.data, accuracy))\n",
        "    if save_model:\n",
        "      torch.save(model, \"model\")\n",
        "      run = wandb.init(job_type=\"model-creation\")\n",
        "      artifact = wandb.Artifact('pretrained-model'+model_label, type='model')\n",
        "      artifact.add_file(\"model\")\n",
        "      run.log_artifact(artifact)\n",
        "else:\n",
        "    run = wandb.init(job_type=\"model-training\")\n",
        "    artifact = run.use_artifact('pretrained-model:latest')\n",
        "    artifact_dir = artifact.download()\n",
        "    print(artifact_dir)\n",
        "    # IF YOU ARE USING GPU THEN YOU NEED TO DISABLE THIS\n",
        "    #CUDA_LAUNCH_BLOCKING=1\n",
        "    if torch.cuda.is_available():\n",
        "      model = torch.load(artifact_dir+\"/model\")\n",
        "    else:\n",
        "      model = torch.load(artifact_dir+\"/model\", map_location=torch.device('cpu'))\n",
        "\n",
        "total = 0\n",
        "correct = 0\n",
        "labels_list = []\n",
        "predictions_list = []\n",
        "for images, labels in test_loader:\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    labels_list.append(labels)\n",
        "    test = Variable(images.view(b_size, 1, 28, 28))\n",
        "    outputs = model(test)\n",
        "    predictions = torch.max(outputs, 1)[1].to(device)\n",
        "    predictions_list.append(predictions)\n",
        "    correct += (predictions == labels).sum()\n",
        "    total += len(labels)\n",
        "accuracy = correct * 100 / total\n",
        "print(accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zua_xNIq2tN4"
      },
      "outputs": [],
      "source": [
        "#@title LeNet { form-width: \"200px\" }\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LeNet, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 6, \n",
        "                           kernel_size = 5, stride = 1, padding = 0)\n",
        "    self.conv2 = nn.Conv2d(in_channels = 6, out_channels = 16, \n",
        "                           kernel_size = 5, stride = 1, padding = 0)\n",
        "    self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 120, \n",
        "                           kernel_size = 5, stride = 1, padding = 0)\n",
        "    self.linear1 = nn.Linear(120, 84)\n",
        "    self.linear2 = nn.Linear(84, 10)\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.avgpool = nn.AvgPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.tanh(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.tanh(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.tanh(x)\n",
        "    \n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    x = self.linear1(x)\n",
        "    x = self.tanh(x)\n",
        "    x = self.linear2(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "model = LeNet().to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "pretrained_model = True #@param {type: \"boolean\"}\n",
        "save_model = False #@param {type: \"boolean\"}\n",
        "model_label = \"v2\" #@param {type: \"string\"}\n",
        "\n",
        "\n",
        "if not pretrained_model:\n",
        "  epochs = 20\n",
        "  train_loss, val_loss = [], []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    \n",
        "      total_train_loss = 0\n",
        "      total_val_loss = 0\n",
        "\n",
        "      model.train()\n",
        "      \n",
        "      # training our model\n",
        "      for idx, (image, label) in enumerate(trainloader):\n",
        "\n",
        "          image, label = image.to(device), label.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          pred = model(image)\n",
        "\n",
        "          loss = criterion(pred, label)\n",
        "          total_train_loss += loss.item()\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      total_train_loss = total_train_loss / (idx + 1)\n",
        "      train_loss.append(total_train_loss)\n",
        "      \n",
        "      # validating our model\n",
        "      model.eval()\n",
        "      total = 0\n",
        "      for idx, (image, label) in enumerate(testloader):\n",
        "          image, label = image.to(device), label.to(device)\n",
        "          pred = model(image)\n",
        "          loss = criterion(pred, label)\n",
        "          total_val_loss += loss.item()\n",
        "\n",
        "          pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "          for i, p in enumerate(pred):\n",
        "              if label[i] == torch.max(p.data, 0)[1]:\n",
        "                  total = total + 1\n",
        "\n",
        "      accuracy = total / test_data_size\n",
        "\n",
        "      total_val_loss = total_val_loss / (idx + 1)\n",
        "      val_loss.append(total_val_loss)\n",
        "\n",
        "      if epoch % 5 == 0:\n",
        "        print('\\nEpoch: {}/{}, Train Loss: {:.4f}, Val Loss: {:.4f}, Val Acc: {:.4f}'.format(epoch, epochs, total_train_loss, total_val_loss, accuracy))\n",
        "      \n",
        "  if save_model:\n",
        "    torch.save(model, \"model\")\n",
        "    run = wandb.init(project=\"SparseAdv\", job_type=\"model-creation\")\n",
        "    artifact = wandb.Artifact('LeNet2-model'+model_label, type='model')\n",
        "    artifact.add_file(\"model\")\n",
        "    run.log_artifact(artifact)\n",
        "\n",
        "else:\n",
        "    run = wandb.init(project=\"SparseAdv\", job_type=\"model-training\")\n",
        "    artifact = run.use_artifact('LeNet2-modelv2:latest')\n",
        "    artifact_dir = artifact.download()\n",
        "    print(artifact_dir)\n",
        "    # IF YOU ARE USING GPU THEN YOU NEED TO DISABLE THIS\n",
        "    #CUDA_LAUNCH_BLOCKING=1\n",
        "    if torch.cuda.is_available():\n",
        "      model = torch.load(artifact_dir+\"/model\")\n",
        "    else:\n",
        "      model = torch.load(artifact_dir+\"/model\", map_location=torch.device('cpu'))\n",
        "\n",
        "\n",
        "# validating our model\n",
        "total = 0\n",
        "correct = 0\n",
        "labels_list = []\n",
        "predictions_list = []\n",
        "\n",
        "transform = transforms.Compose([\n",
        "          transforms.Resize((32, 32)),\n",
        "          transforms.ToTensor()\n",
        "          ])\n",
        "\n",
        "\n",
        "test_set = datasets.MNIST('DATA_MNIST/', download=True, train=False, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=b_size, shuffle=True)\n",
        "\n",
        "for images, labels in test_loader: #test_loader\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    labels_list.append(labels)\n",
        "    outputs = model(images)\n",
        "    predictions = torch.max(outputs, 1)[1].to(device)\n",
        "    predictions_list.append(predictions)\n",
        "    correct += (predictions == labels).sum()\n",
        "    total += len(labels)\n",
        "accuracy = correct * 100 / total\n",
        "print(accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBn7LcbD3M2i"
      },
      "outputs": [],
      "source": [
        "#@title VGG { form-width: \"200px\" }\n",
        "\n",
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "shuffle = False #@param {type: \"boolean\"}\n",
        "#@markdown Batch size\n",
        "b_size =  100#@param {type: \"integer\"}\n",
        "test_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, train=False, transform=\n",
        "                                               transforms.Compose([transforms.ToTensor()])) \n",
        "test_loader = torch.utils.data.DataLoader(test_set,\n",
        "                                          batch_size=b_size, shuffle=shuffle, drop_last=True)\n",
        "pretrained_model = True #@param {type: \"boolean\"}\n",
        "save_model = False #@param {type: \"boolean\"}\n",
        "model_label = \"v0\" #@param {type: \"string\"}\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "num_epochs = 80\n",
        "learning_rate = 0.01\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Image preprocessing modules\n",
        "transform = transforms.Compose([\n",
        "    transforms.Pad(4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32),\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "#######\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "########\n",
        "\n",
        "# CIFAR-10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
        "                                             train=True, \n",
        "                                             transform=transform_train,\n",
        "                                             download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
        "                                            train=False, \n",
        "                                            transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=100, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=100, \n",
        "                                          shuffle=False)\n",
        "\n",
        "# 3x3 convolution\n",
        "def conv3x3(in_channels, out_channels, stride=1):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
        "                     stride=stride, padding=1, bias=False)\n",
        "\n",
        "# Residual block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(out_channels, out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.downsample:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# ResNet\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 16\n",
        "        self.conv = conv3x3(3, 16)\n",
        "        self.bn = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
        "        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
        "        self.avg_pool = nn.AvgPool2d(8)\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if (stride != 1) or (self.in_channels != out_channels):\n",
        "            downsample = nn.Sequential(\n",
        "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels))\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "model = VGG('VGG19').to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "# For updating learning rate\n",
        "def update_lr(optimizer, lr):    \n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "if not pretrained_model:\n",
        "    # Train the model\n",
        "    total_step = len(train_loader)\n",
        "    curr_lr = learning_rate\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i+1) % 100 == 0:\n",
        "                print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
        "                      .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "        # Decay learning rate\n",
        "        if (epoch+1) % 20 == 0:\n",
        "            curr_lr /= 3\n",
        "            update_lr(optimizer, curr_lr)\n",
        "\n",
        "    if save_model:\n",
        "        torch.save(model, \"model\")\n",
        "        run = wandb.init(job_type=\"model-creation\")\n",
        "        artifact = wandb.Artifact('VGG_new-model2'+model_label, type='model')\n",
        "        artifact.add_file(\"model\")\n",
        "        run.log_artifact(artifact)\n",
        "\n",
        "else:\n",
        "    run = wandb.init(job_type=\"model-training\")\n",
        "    artifact = run.use_artifact('VGG_new-model2v0:latest')\n",
        "    artifact_dir = artifact.download()\n",
        "    print(artifact_dir)\n",
        "    # IF YOU ARE USING GPU THEN YOU NEED TO DISABLE THIS\n",
        "    #CUDA_LAUNCH_BLOCKING=1\n",
        "    if torch.cuda.is_available():\n",
        "      model = torch.load(artifact_dir+\"/model\")\n",
        "    else:\n",
        "      model = torch.load(artifact_dir+\"/model\", map_location=torch.device('cpu'))\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc4Sl--y4UTh"
      },
      "source": [
        "# ------ Sparse Adversarial Attacks ------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd-UoATp4hbK"
      },
      "source": [
        "# SparseFool\n",
        "code from the SF paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TWUgv4nx4Ybj"
      },
      "outputs": [],
      "source": [
        "#@title DF subroutine { form-width: \"250px\" }\n",
        "\n",
        "def deepfool(\n",
        "    im,                   # image\n",
        "    net,                  # target network\n",
        "    lambda_fac=3.,        # control parameter lambda\n",
        "    num_classes=10,       # number of classes\n",
        "    overshoot=0.02,       # \"eta\" in thesis\n",
        "    max_iter=50,          # maximum number of iterations\n",
        "    device='cuda'\n",
        "):\n",
        "    '''\n",
        "    outputs the approximated decision boundary and boundary point\n",
        "    This function implements the DeepFool subroutine for computing \n",
        "    adversarial perturbations. It is used by sparsefool.py for the \n",
        "    linear approximation of the decision boundary.\n",
        "    '''\n",
        "    \n",
        "    image = copy.deepcopy(im)\n",
        "    input_shape = image.size()\n",
        "\n",
        "    f_image = net.forward(Variable(image, requires_grad=True)).data.cpu().numpy().flatten()\n",
        "    I = (np.array(f_image)).flatten().argsort()[::-1]\n",
        "    I = I[0:num_classes]\n",
        "    label = I[0]\n",
        "\n",
        "    pert_image = copy.deepcopy(image)\n",
        "    r_tot = torch.zeros(input_shape).to(device)\n",
        "\n",
        "    k_i = label\n",
        "    loop_i = 0\n",
        "\n",
        "    while k_i == label and loop_i < max_iter:\n",
        "\n",
        "        x = Variable(pert_image, requires_grad=True)\n",
        "        fs = net.forward(x)\n",
        "\n",
        "        pert = torch.Tensor([np.inf])[0].to(device)\n",
        "        w = torch.zeros(input_shape).to(device)\n",
        "\n",
        "        fs[0, I[0]].backward(retain_graph=True)\n",
        "        grad_orig = copy.deepcopy(x.grad.data)\n",
        "\n",
        "        for k in range(1, num_classes):\n",
        "            x.grad.zero_()\n",
        "\n",
        "            fs[0, I[k]].backward(retain_graph=True)\n",
        "            cur_grad = copy.deepcopy(x.grad.data)\n",
        "\n",
        "            w_k = cur_grad - grad_orig\n",
        "            f_k = (fs[0, I[k]] - fs[0, I[0]]).data\n",
        "\n",
        "            pert_k = torch.abs(f_k) / w_k.norm()\n",
        "\n",
        "            if pert_k < pert:\n",
        "                pert = pert_k + 0.\n",
        "                w = w_k + 0.\n",
        "\n",
        "        r_i = torch.clamp(pert, min=1e-4) * w / w.norm()\n",
        "        r_tot = r_tot + r_i\n",
        "\n",
        "        pert_image = pert_image + r_i\n",
        "\n",
        "        check_fool = image + (1 + overshoot) * r_tot\n",
        "        k_i = torch.argmax(net.forward(Variable(check_fool, requires_grad=True)).data).item()\n",
        "\n",
        "        loop_i += 1\n",
        "\n",
        "    x = Variable(pert_image, requires_grad=True)\n",
        "    fs = net.forward(x)\n",
        "    (fs[0, k_i] - fs[0, label]).backward(retain_graph=True)\n",
        "    grad = copy.deepcopy(x.grad.data)\n",
        "    grad = grad / grad.norm()\n",
        "\n",
        "    r_tot = lambda_fac * r_tot\n",
        "    pert_image = image + r_tot\n",
        "\n",
        "    return grad, pert_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UZlpaDXX44il"
      },
      "outputs": [],
      "source": [
        "#@title Linear Solver subroutine { form-width: \"250px\" }\n",
        "def linear_solver(\n",
        "    x_0,              # initial image \n",
        "    normal,           # normal computed by DeepFool previously\n",
        "    boundary_point,   # boundary point approximated by DeepFool\n",
        "    lb,               # lower bound on pixel values\n",
        "    ub                # upper bound on pixel values\n",
        "):\n",
        "    '''\n",
        "    outputs next iterate for SF \n",
        "    This function implements the Linear Solver subroutine for solving the \n",
        "    linearized box-constrained problem. \n",
        "    It is used by sparsefool.py for solving the linearized problem.\n",
        "    '''\n",
        "\n",
        "    input_shape = x_0.size()\n",
        "\n",
        "    coord_vec = copy.deepcopy(normal)\n",
        "    plane_normal = copy.deepcopy(coord_vec).view(-1)\n",
        "    plane_point = copy.deepcopy(boundary_point).view(-1)\n",
        "\n",
        "    x_i = copy.deepcopy(x_0)\n",
        "\n",
        "    f_k = torch.dot(plane_normal, x_0.view(-1) - plane_point)\n",
        "    sign_true = f_k.sign().item()\n",
        "\n",
        "    beta = 0.001 * sign_true\n",
        "    current_sign = sign_true\n",
        "\n",
        "    while current_sign == sign_true and coord_vec.nonzero().size()[0] > 0:\n",
        "\n",
        "        f_k = torch.dot(plane_normal, x_i.view(-1) - plane_point) + beta\n",
        "\n",
        "        pert = f_k.abs() / coord_vec.abs().max()\n",
        "\n",
        "        mask = torch.zeros_like(coord_vec)\n",
        "        coord_vec = coord_vec.cpu()\n",
        "        mask[np.unravel_index( torch.argmax (coord_vec.abs() ), input_shape)] = 1.\n",
        "        coord_vec = coord_vec.to(device)\n",
        "\n",
        "        r_i = torch.clamp(pert, min=1e-4) * mask * coord_vec.sign()\n",
        "        x_i = x_i + r_i\n",
        "        x_i = clip_image_values(x_i, lb, ub)\n",
        "\n",
        "        f_k = torch.dot(plane_normal, x_i.view(-1) - plane_point)\n",
        "        current_sign = f_k.sign().item()\n",
        "\n",
        "        coord_vec[r_i != 0] = 0\n",
        "\n",
        "    return x_i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "56tgf4Mu6auF"
      },
      "outputs": [],
      "source": [
        "#@title SparseFool code { form-width: \"250px\" }\n",
        "\n",
        "import torch as torch\n",
        "import copy\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def sparsefool(\n",
        "    x_0,                # image\n",
        "    net,                # target network\n",
        "    lb,                 # lower bound on pixel values\n",
        "    ub,                 # upper bound on pixel values\n",
        "    lambda_=3.,         # control parameter lambda\n",
        "    max_iter=20,        # maximum number of iterations\n",
        "    epsilon=0.02,       # \"eta\" in thesis (overshoot)\n",
        "    device='cuda'\n",
        "):\n",
        "    '''\n",
        "    outputs: \n",
        "    fool_im:    adversarial example\n",
        "    r:          perturbation \n",
        "    pred_label: models prediction \n",
        "    loops:      number of iterations that was needed\n",
        "    '''\n",
        "\n",
        "    pred_label = torch.argmax(net(x_0))\n",
        "\n",
        "    x_i = copy.deepcopy(x_0)\n",
        "    fool_im = copy.deepcopy(x_i)\n",
        "\n",
        "    fool_label = pred_label\n",
        "    loops = 0\n",
        "\n",
        "    while fool_label == pred_label and loops < max_iter:\n",
        "\n",
        "        normal, x_adv = deepfool(x_i, net, lambda_, device=device)\n",
        "\n",
        "        x_i = linear_solver(x_i, normal, x_adv, lb, ub)\n",
        "\n",
        "        fool_im = x_0 + (1 + epsilon) * (x_i - x_0)\n",
        "        fool_im = clip_image_values(fool_im, lb, ub)\n",
        "        fool_label = torch.argmax(net.forward(Variable(fool_im, requires_grad=True)).data).item()\n",
        "\n",
        "        loops += 1\n",
        "\n",
        "    r = fool_im - x_0\n",
        "    return fool_im, r, pred_label, fool_label, loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Qz2R9wkj6rj5"
      },
      "outputs": [],
      "source": [
        "#@title Utils { form-width: \"250px\" }\n",
        "\n",
        "def clip_image_values(x, minv, maxv): \n",
        "    x = torch.clamp(x,minv,maxv)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkk0K7Z87EaL"
      },
      "source": [
        "# CornerSearch\n",
        "original code from the CS paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EGVAGsO37JpL"
      },
      "outputs": [],
      "source": [
        "#@title CS utils { form-width: \"250px\" }\n",
        "\n",
        "def get_logits(model, x_nat):\n",
        "    x = torch.from_numpy(x_nat).permute(0, 3, 1, 2).float()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        output = model(x.cuda())\n",
        "    \n",
        "    return output.cpu().numpy()\n",
        "\n",
        "def get_predictions(model, x_nat, y_nat):\n",
        "    x = torch.from_numpy(x_nat).permute(0, 3, 1, 2).float()\n",
        "    y = torch.from_numpy(y_nat)\n",
        "    with torch.no_grad():\n",
        "        output = model(x.cuda())\n",
        "    \n",
        "    return (output.cpu().max(dim=-1)[1] == y).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iSM2hvPw7PyH"
      },
      "outputs": [],
      "source": [
        "#@title CornerSearch code { form-width: \"250px\" }\n",
        "\n",
        "def onepixel_perturbation(attack, orig_x, pos, sigma):\n",
        "  ''' \n",
        "  returns a batch with the possible perturbations of the pixel in position pos \n",
        "  '''\n",
        "    \n",
        "  if attack.type_attack == 'L0':\n",
        "    if orig_x.shape[-1] == 3:\n",
        "      batch_x = np.tile(orig_x,(8,1,1,1))\n",
        "      t = np.zeros([3])\n",
        "      for counter in range(8):\n",
        "        t2 = counter + 0\n",
        "        for c in range(3):\n",
        "          t[c] = t2 % 2\n",
        "          t2 = (t2 - t[c])/2\n",
        "        batch_x[counter,pos[0],pos[1]] = t.astype(np.float32)\n",
        "    elif orig_x.shape[-1] == 1:\n",
        "      batch_x = np.tile(orig_x,(2,1,1,1))\n",
        "      batch_x[0,pos[0],pos[1],0] = 0.0\n",
        "      batch_x[1,pos[0],pos[1],0] = 1.0\n",
        "  \n",
        "  elif attack.type_attack == 'L0+Linf':\n",
        "    if orig_x.shape[-1] == 3:\n",
        "      batch_x = np.tile(orig_x,(8,1,1,1))\n",
        "      t = np.zeros([3])\n",
        "      for counter in range(8):\n",
        "        t2 = counter + 0\n",
        "        for c in range(3):\n",
        "          t3 = t2 % 2\n",
        "          t[c] = (t3*2.0 - 1.0)*attack.epsilon\n",
        "          t2 = (t2 - t3)/2\n",
        "        batch_x[counter,pos[0],pos[1]] = np.clip(t.astype(np.float32) + orig_x[pos[0],pos[1]], 0.0, 1.0)\n",
        "    elif orig_x.shape[-1] == 1:\n",
        "      batch_x = np.tile(orig_x,(2,1,1,1))\n",
        "      batch_x[0,pos[0],pos[1],0] = np.clip(batch_x[0,pos[0],pos[1],0] - attack.epsilon, 0.0, 1.0)\n",
        "      batch_x[1,pos[0],pos[1],0] = np.clip(batch_x[1,pos[0],pos[1],0] + attack.epsilon, 0.0, 1.0)\n",
        "  \n",
        "  elif attack.type_attack == 'L0+sigma':\n",
        "    batch_x = np.tile(orig_x,(2,1,1,1))\n",
        "    if orig_x.shape[-1] == 3:\n",
        "      batch_x[0,pos[0],pos[1]] = np.clip(batch_x[0,pos[0],pos[1]]*(1.0 - attack.kappa*sigma[pos[0],pos[1]]), 0.0, 1.0)\n",
        "      batch_x[1,pos[0],pos[1]] = np.clip(batch_x[0,pos[0],pos[1]]*(1.0 + attack.kappa*sigma[pos[0],pos[1]]), 0.0, 1.0)\n",
        "    \n",
        "    elif orig_x.shape[-1] == 1:\n",
        "      batch_x[0,pos[0],pos[1]] = np.clip(batch_x[0,pos[0],pos[1]] - attack.kappa*sigma[pos[0],pos[1]], 0.0, 1.0)\n",
        "      batch_x[1,pos[0],pos[1]] = np.clip(batch_x[0,pos[0],pos[1]] + attack.kappa*sigma[pos[0],pos[1]], 0.0, 1.0)\n",
        "    \n",
        "  else:\n",
        "    raise ValueError('unknown attack')\n",
        "  \n",
        "  return batch_x\n",
        "    \n",
        "def onepixel_perturbation_image(attack, orig_x, sigma):\n",
        "  ''' returns a batch with all the possible perturbations of the image orig_x '''\n",
        "  \n",
        "  n_channels = orig_x.shape[-1]\n",
        "  assert n_channels in [1, 3]\n",
        "  n_corners = 2**n_channels if attack.type_attack in ['L0', 'L0+Linf'] else 2\n",
        "  \n",
        "  batch_x = np.zeros([n_corners*orig_x.shape[0]*orig_x.shape[1], orig_x.shape[0], orig_x.shape[1], orig_x.shape[2]])\n",
        "  for counter in range(orig_x.shape[0]):\n",
        "      for counter2 in range(orig_x.shape[1]):\n",
        "        batch_x[(counter*orig_x.shape[0]+counter2)*n_corners:(counter*orig_x.shape[1]+counter2)*n_corners+n_corners] = np.clip(onepixel_perturbation(attack, orig_x, [counter,counter2], sigma), 0.0, 1.0)\n",
        "  \n",
        "  return batch_x\n",
        "\n",
        "def flat2square(attack, ind):\n",
        "  ''' returns the position and the perturbation given the index of an image\n",
        "      of the batch of all the possible perturbations '''\n",
        "  \n",
        "  if attack.type_attack in ['L0', 'L0+Linf']:\n",
        "    if attack.shape_img[-1] == 3:\n",
        "      new_pixel = ind % 8\n",
        "      ind = (ind - new_pixel)//8\n",
        "      c = ind % attack.shape_img[1]\n",
        "      r = (ind - c)//attack.shape_img[1]\n",
        "      t = np.zeros([ind.shape[0],3])\n",
        "      for counter in range(3):\n",
        "        t[:,counter] = new_pixel % 2\n",
        "        new_pixel = (new_pixel - t[:,counter])/2\n",
        "    elif attack.shape_img[-1] == 1:\n",
        "      t = ind % 2\n",
        "      ind = (ind-t)//2\n",
        "      c = ind % attack.shape_img[1]\n",
        "      r = (ind-c)//attack.shape_img[1]\n",
        "  \n",
        "  elif attack.type_attack == 'L0+sigma':\n",
        "      t = ind % 2\n",
        "      c = ((ind - t)//2) % attack.shape_img[1]\n",
        "      r = ((ind - t)//2 - c)//attack.shape_img[1]\n",
        "    \n",
        "  return r, c, t\n",
        "\n",
        "def npixels_perturbation(attack, orig_x, ind, k, sigma):\n",
        "  ''' creates n_iter images which differ from orig_x in at most k pixels '''\n",
        "  \n",
        "  # sampling the n_iter k-pixels perturbations\n",
        "  ind2 = np.random.randint(0, attack.n_max**2, (attack.n_iter, k))\n",
        "  ind2 = attack.n_max - np.floor(ind2**0.5).astype(int) - 1\n",
        "  \n",
        "  # creating the n_iter k-pixels perturbed images\n",
        "  batch_x = np.tile(orig_x,(attack.n_iter,1,1,1))\n",
        "  if attack.type_attack == 'L0':\n",
        "    for counter in range(attack.n_iter):\n",
        "      p11, p12, d1 = flat2square(attack, ind[ind2[counter]])\n",
        "      batch_x[counter,p11,p12] = d1 + 0 if attack.shape_img[-1] == 3 else np.expand_dims(d1 + 0, 1)\n",
        "  \n",
        "  elif attack.type_attack == 'L0+Linf':\n",
        "    for counter in range(attack.n_iter):\n",
        "      p11, p12, d1 = flat2square(attack, ind[ind2[counter]])\n",
        "      d1 = d1 + 0 if attack.shape_img[-1] == 3 else np.expand_dims(d1 + 0, 1)\n",
        "      batch_x[counter,p11,p12] = np.clip(batch_x[counter,p11,p12]+(2.0*d1 - 1.0)*attack.epsilon, 0.0, 1.0)\n",
        "  \n",
        "  elif attack.type_attack == 'L0+sigma':\n",
        "    for counter in range(attack.n_iter):\n",
        "      p11, p12, d1 = flat2square(attack, ind[ind2[counter]])\n",
        "      d1 = np.expand_dims(d1,1)\n",
        "      if attack.shape_img[-1] == 3: batch_x[counter,p11,p12] = np.clip(batch_x[counter,p11,p12] - attack.kappa*sigma[p11,p12]*(1-d1) + attack.kappa*sigma[p11,p12]*d1, 0.0, 1.0)\n",
        "      elif attack.shape_img[-1] == 1: batch_x[counter,p11,p12] = np.clip(batch_x[counter,p11,p12] - attack.kappa*sigma[p11,p12]*(1-d1) + attack.kappa*sigma[p11,p12]*d1, 0.0, 1.0)\n",
        "      \n",
        "  return batch_x\n",
        "\n",
        "def sigma_map(x):\n",
        "  ''' creates the sigma-map for the batch x '''\n",
        "  \n",
        "  sh = [4]\n",
        "  sh.extend(x.shape)\n",
        "  t = np.zeros(sh)\n",
        "  t[0,:,:-1] = x[:,1:]\n",
        "  t[0,:,-1] = x[:,-1]\n",
        "  t[1,:,1:] = x[:,:-1]\n",
        "  t[1,:,0] = x[:,0]\n",
        "  t[2,:,:,:-1] = x[:,:,1:]\n",
        "  t[2,:,:,-1] = x[:,:,-1]\n",
        "  t[3,:,:,1:] = x[:,:,:-1]\n",
        "  t[3,:,:,0] = x[:,:,0]\n",
        "\n",
        "  mean1 = (t[0] + x + t[1])/3\n",
        "  sd1 = np.sqrt(((t[0]-mean1)**2 + (x-mean1)**2 + (t[1]-mean1)**2)/3)\n",
        "\n",
        "  mean2 = (t[2] + x + t[3])/3\n",
        "  sd2 = np.sqrt(((t[2]-mean2)**2 + (x-mean2)**2 + (t[3]-mean2)**2)/3)\n",
        "\n",
        "  sd = np.minimum(sd1, sd2)\n",
        "  sd = np.sqrt(sd)\n",
        "  \n",
        "  return sd\n",
        "  \n",
        "class CSattack():\n",
        "  def __init__(self, model, args):\n",
        "    self.model = model\n",
        "    self.type_attack = args['type_attack'] # 'L0', 'L0+Linf', 'L0+sigma'\n",
        "    self.n_iter = args['n_iter']           # number of iterations (N_iter in the paper)\n",
        "    self.n_max = args['n_max']             # the modifications for k-pixels perturbations are sampled among the best n_max (N in the paper)\n",
        "    self.epsilon = args['epsilon']         # for L0+Linf, the bound on the Linf-norm of the perturbation\n",
        "    self.kappa = args['kappa']             # for L0+sigma (see kappa in the paper), larger kappa means easier and more visible attacks\n",
        "    self.k = args['sparsity']              # maximum number of pixels that can be modified (k_max in the paper)\n",
        "    self.size_incr = args['size_incr']     # size of progressive increment of sparsity levels to check  \n",
        "  \n",
        "  def perturb(self, x_nat, y_nat):\n",
        "    adv = np.copy(x_nat)\n",
        "    fl_success = np.ones([x_nat.shape[0]])\n",
        "    self.shape_img = x_nat.shape[1:]\n",
        "    self.sigma = sigma_map(x_nat)\n",
        "    self.n_classes = 10\n",
        "    self.n_corners = 2**self.shape_img[2] if self.type_attack in ['L0', 'L0+Linf'] else 2\n",
        "    #corr_pred = sess.run(self.model.correct_prediction, {self.model.x_input: x_nat, self.model.y_input: y_nat})\n",
        "    corr_pred = get_predictions(self.model, x_nat, y_nat)\n",
        "    #print(\"corr pred \"+str(corr_pred))\n",
        "    bs = self.shape_img[0]*self.shape_img[1]\n",
        "    \n",
        "    for c in range(x_nat.shape[0]):\n",
        "      if corr_pred[c]:\n",
        "        sigma = np.copy(self.sigma[c])\n",
        "        batch_x = onepixel_perturbation_image(self, x_nat[c], sigma)\n",
        "        batch_y = np.squeeze(y_nat[c])\n",
        "        logit_2 = np.zeros([batch_x.shape[0], self.n_classes])\n",
        "        found = False\n",
        "        \n",
        "        # checks one-pixels modifications\n",
        "        for counter in range(self.n_corners):\n",
        "          #logit_2[counter*bs:(counter+1)*bs], pred = sess.run([self.model.y, self.model.correct_prediction], feed_dict={self.model.x_input: batch_x[counter*bs:(counter+1)*bs], self.model.y_input: np.tile(batch_y,(bs))})\n",
        "          logit_2[counter*bs:(counter+1)*bs] = get_logits(self.model, batch_x[counter*bs:(counter+1)*bs])\n",
        "          pred = logit_2[counter*bs:(counter+1)*bs].argmax(axis=-1) == np.tile(batch_y,(bs))\n",
        "          if not pred.all() and not found:\n",
        "            ind_adv = np.where(pred.astype(int)==0)\n",
        "            adv[c] = batch_x[counter*bs + ind_adv[0][0]]\n",
        "            found = True\n",
        "            print('Point {} - adversarial example found changing 1 pixel'.format(c))\n",
        "        \n",
        "        # creates the orderings\n",
        "        t1 = np.copy(logit_2[:, batch_y])\n",
        "        logit_2[:, batch_y] = -1000.0*np.ones(np.shape(logit_2[:, batch_y]))\n",
        "        t2 = np.amax(logit_2, axis=1)\n",
        "        t3 = t1 - t2\n",
        "        logit_3 = np.tile(np.expand_dims(t1,axis=1),(1,self.n_classes))-logit_2\n",
        "        logit_3[:, batch_y] = t3\n",
        "        ind = np.argsort(logit_3, axis=0)\n",
        "        \n",
        "        # checks multiple-pixels modifications\n",
        "        for n3 in range(1 + self.size_incr, self.k + 1, self.size_incr):\n",
        "          if not found:\n",
        "             for c2 in range(self.n_classes):\n",
        "               if not found:\n",
        "                 ind_cl = np.copy(ind[:, c2])\n",
        "\n",
        "                 batch_x = npixels_perturbation(self, x_nat[c], ind_cl, n3, sigma)\n",
        "                 #pred = sess.run(self.model.correct_prediction, feed_dict={self.model.x_input: batch_x, self.model.y_input: np.tile(batch_y,(batch_x.shape[0]))})\n",
        "                 pred = get_predictions(self.model, batch_x, np.tile(batch_y,(batch_x.shape[0])))\n",
        "                 #print(\"pred \"+str(pred))\n",
        "\n",
        "                 if np.sum(pred.astype(np.int32)) < self.n_iter and not found:\n",
        "                   found = True\n",
        "                   ind_adv = np.where(pred.astype(int)==0)\n",
        "                   adv[c] = batch_x[ind_adv[0][0]]\n",
        "                   print('Point {} - adversarial example found changing {} pixels'.format(c, np.sum(np.amax(np.abs(adv[c] - x_nat[c]) > 1e-10, axis=-1), axis=(0,1))))\n",
        "        \n",
        "        if not found:\n",
        "          fl_success[c] = 0\n",
        "          print('Point {} - adversarial example not found'.format(c))\n",
        "      \n",
        "      else:\n",
        "        print('Point {} - misclassified'.format(c))\n",
        "    \n",
        "    pixels_changed = np.sum(np.amax(np.abs(adv - x_nat) > 1e-10, axis=-1), axis=(1,2))\n",
        "    #print('Pixels changed: ', pixels_changed)\n",
        "    corr_pred = get_predictions(self.model, adv, y_nat)\n",
        "    predictions = model(torch.tensor(adv).permute(0,3,2,1).to(device))\n",
        "    print('Robust accuracy at {} pixels: {:.2f}%'.format(self.k, np.sum(corr_pred)/x_nat.shape[0]*100.0))\n",
        "    print('Maximum perturbation size: {:.5f}'.format(np.amax(np.abs(adv - x_nat))))\n",
        "    \n",
        "    return adv, pixels_changed, fl_success"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6TWg2n27gGd"
      },
      "source": [
        "# JSMA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "he3M0iUd7ptl"
      },
      "outputs": [],
      "source": [
        "#@title JSMA code { form-width: \"150px\" }\n",
        "\n",
        "\n",
        "def compute_jacobian_batch(\n",
        "    inputs,           # input image\n",
        "    output            # the models' output\n",
        "):\n",
        "    # outputs the forward derivative\n",
        "\n",
        "    assert inputs.requires_grad\n",
        "\n",
        "    num_classes = output.size()[1]\n",
        "\n",
        "    jacobian = torch.zeros(num_classes, *inputs.size())\n",
        "    grad_output = torch.zeros(*output.size())\n",
        "    if inputs.is_cuda:\n",
        "        grad_output = grad_output.cuda()\n",
        "        jacobian = jacobian.cuda()\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        if inputs.grad != None:\n",
        "          inputs.grad.zero_()\n",
        "        grad_output.zero_()\n",
        "        grad_output[:, i] = 1\n",
        "        output.backward(grad_output, retain_graph=True)\n",
        "        jacobian[i] = inputs.grad.data\n",
        "    return torch.transpose(jacobian, dim0=0, dim1=1)\n",
        "\n",
        "def saliency_map_batch(\n",
        "    jacobian,             # jacobian computed by compute_jacobian_batch\n",
        "    search_space,         # \"Gamma\" in thesis\n",
        "    target_index,         # target class \n",
        "    increasing=True       # computing increasing or decreasing saliency map \n",
        "):\n",
        "    # outputs the maximum value of the saliency map and its indices\n",
        "\n",
        "    all_sum = torch.sum(jacobian, 1).squeeze() # compute sum over all classes \n",
        "    alpha = jacobian[0][target_index]    \n",
        "    beta = torch.flatten(all_sum - alpha)              \n",
        "    alpha = torch.flatten(alpha) \n",
        "    num_pixels = jacobian.shape[2]*jacobian.shape[3]\n",
        "\n",
        "    alpha_p = torch.ones(num_pixels,num_pixels)\n",
        "    alpha_p = torch.triu(alpha_p)\n",
        "    alpha_p = alpha_p.fill_diagonal_(0).to(device)\n",
        "\n",
        "    alpha_p = alpha_p * alpha.to(device)\n",
        "\n",
        "    tmp = torch.reshape(alpha,(len(alpha),1))\n",
        "    tmp_mtrx = torch.ones(num_pixels,num_pixels).to(device)\n",
        "    tmp_mtrx = tmp_mtrx * tmp\n",
        "    alpha_q = torch.triu(tmp_mtrx)\n",
        "    alpha_q = alpha_q.fill_diagonal_(0) \n",
        "\n",
        "    alpha_pq = alpha_p + alpha_q\n",
        "\n",
        "    beta_p = torch.ones(num_pixels,num_pixels)\n",
        "    beta_p = torch.triu(beta_p)\n",
        "    beta_p = beta_p.fill_diagonal_(0).to(device)\n",
        "\n",
        "    beta_p = beta_p * beta\n",
        "\n",
        "    tmp = torch.reshape(beta,(len(beta),1))\n",
        "    tmp_mtrx = torch.ones(num_pixels,num_pixels).to(device)\n",
        "    tmp_mtrx = tmp_mtrx * tmp\n",
        "    beta_q = torch.triu(tmp_mtrx)\n",
        "    beta_q = beta_q.fill_diagonal_(0) \n",
        "\n",
        "    beta_pq = beta_p + beta_q\n",
        "\n",
        "    if increasing:\n",
        "        mask1 = torch.ge(alpha_pq, 0.0)    # alpha > 0, \n",
        "        mask2 = torch.le(beta_pq, 0.0)     # beta < 0    for increasing saliency map\n",
        "    else:\n",
        "        mask1 = torch.le(alpha_pq, 0.0)    # alpha < 0, \n",
        "        mask2 = torch.ge(beta_pq, 0.0)     # beta > 0    for decreasing saliency map \n",
        "\n",
        "    mask = torch.mul(mask1, mask2) # both constraints fulfilled\n",
        "\n",
        "    space = torch.flatten(search_space) \n",
        "    for i in range (len(space)): \n",
        "    # delte rows and columns corresponding to pixels which are already modified\n",
        "        if space[i] == 0:\n",
        "            mask[i] = torch.zeros_like(mask[i]) \n",
        "            for j in range(mask.shape[0]):\n",
        "                mask[j][i] = 0                  \n",
        "\n",
        "    if increasing:\n",
        "        saliency_map = torch.mul(torch.mul(alpha_pq, torch.abs(beta_pq)), mask.float())   # alpha * abs(beta) where alpha > 0, beta < 0\n",
        "    else:\n",
        "        saliency_map = torch.mul(torch.mul(torch.abs(alpha_pq), beta_pq), mask.float())   # abs(alpha) * beta where alpha < 0, beta > 0\n",
        "    \n",
        "    # search for the maximal value and its indices\n",
        "\n",
        "    max_value = torch.max(saliency_map)\n",
        "    idx = torch.argmax(saliency_map)\n",
        "    p = torch.div(idx, num_pixels, rounding_mode='trunc')\n",
        "    q = idx - (p * num_pixels)\n",
        "\n",
        "    return max_value, (p,q)\n",
        "\n",
        "\n",
        "def jsma_batch(\n",
        "    model,                  # target model\n",
        "    input_tensor,           # images\n",
        "    true_labels,            # true labels\n",
        "    target_class,           # target classes\n",
        "    max_distortion=1        # maximum distortion allowed\n",
        "):\n",
        "    '''\n",
        "    outputs:\n",
        "    input_features: adverasarial images\n",
        "    found:          number of images for which an adversarial was found\n",
        "    dist:           distortions\n",
        "    '''\n",
        "\n",
        "    # Make a clone since we will alter the values\n",
        "    input_features = torch.autograd.Variable(input_tensor.clone(), requires_grad=True) \n",
        "    batch_size = input_features.shape[0]\n",
        "    width = input_features.size(2)  \n",
        "    height = input_features.size(3) \n",
        "    num_pixels = width * height     \n",
        "    max_iter = math.floor((num_pixels*max_distortion)) \n",
        "    count = [0 for i in range(batch_size)]\n",
        "\n",
        "    search_space = torch.ones(batch_size, width, height).byte() \n",
        "    if input_features.is_cuda:\n",
        "        search_space = search_space.cuda()    \n",
        "\n",
        "    output = model(input_features) \n",
        "    _, source_class = torch.max(output.data, 1) \n",
        "\n",
        "    image_done = [False for i in range(batch_size)] \n",
        "    adv = torch.zeros_like(input_tensor)\n",
        "    \n",
        "    while (max(count) < max_iter) and (min(image_done) == False) and (search_space.sum() != 0):\n",
        "        # Calculate Jacobian\n",
        "        jacobian = compute_jacobian_batch(input_features, output) \n",
        "\n",
        "        for i in range(batch_size):\n",
        "            if image_done[i] != True:\n",
        "                image_jacobian = jacobian[i].permute(1,0,2,3) \n",
        "\n",
        "                # Compute increasing saliency map and find the maximum value and its indices \n",
        "                s_plus_value, s_plus_index = saliency_map_batch(image_jacobian, search_space[i], target_class[i], increasing=True) \n",
        "\n",
        "                # Compute decreasing saliency map and find the maximum value and its indices  \n",
        "                s_minus_value, s_minus_index = saliency_map_batch(image_jacobian, search_space[i], target_class[i], increasing=False)\n",
        "\n",
        "                # if not zero: from increasing and decreasing pixel pick the one with bigger impact\n",
        "                if s_plus_value == 0.0 and s_minus_value == 0.0:\n",
        "                    print(\"No pair of pixels fulfills the conditions.\")\n",
        "                    return input_features, found, dist\n",
        "        \n",
        "                if s_plus_value > s_minus_value:\n",
        "                    p = s_plus_index[0]\n",
        "                    q = s_plus_index[1]\n",
        "\n",
        "                    # modify pixel p and q \n",
        "                    #p_0 = (p // width) \n",
        "                    p_0 = torch.div(p, width, rounding_mode='trunc')\n",
        "                    p_1 = p - (p_0 * width)\n",
        "                    #q_0 = (q // width) \n",
        "                    q_0 = torch.div(q, width, rounding_mode='trunc')\n",
        "                    q_1 = q - (q_0 * width)\n",
        "                    input_features.data[i][0][p_0][p_1] =1 \n",
        "                    input_features.data[i][0][q_0][q_1] =1 \n",
        "                    # remove modifyed pixel from search space\n",
        "                    search_space[i][p_0][p_1] = 0\n",
        "                    search_space[i][q_0][q_1] = 0\n",
        "\n",
        "                else:\n",
        "                    p = s_minus_index[0]\n",
        "                    q = s_minus_index[1]\n",
        "                    # modify pixel p and q \n",
        "                    #p_0 = (p // width) \n",
        "                    p_0 = torch.div(p, width, rounding_mode='trunc')\n",
        "                    p_1 = p - (p_0 * width)\n",
        "                    #q_0 = (q // width) \n",
        "                    q_0 = torch.div(q, width, rounding_mode='trunc')\n",
        "                    q_1 = q - (q_0 * width)\n",
        "                    input_features.data[i][0][p_0][p_1] =0 # +=1\n",
        "                    input_features.data[i][0][q_0][q_1] =0 # +=1\n",
        "                    # remove modifyed pixel from search space\n",
        "                    search_space[i][p_0][p_1] = 0\n",
        "                    search_space[i][q_0][q_1] = 0\n",
        "\n",
        "                count[i] += 2\n",
        "            \n",
        "            _, source_class = torch.max(output.data, 1) \n",
        "\n",
        "            for i in range(batch_size):\n",
        "                if image_done[i] != True:\n",
        "                    if source_class[i] == target_class[i]:\n",
        "                        image_done[i] = True\n",
        "                        adv[i] = input_features[i]\n",
        "            output = model(input_features) \n",
        "\n",
        "    dist =[]\n",
        "    found = 0\n",
        "    for i in range(batch_size):\n",
        "        if count[i] >= max_iter:\n",
        "            print(\"Reached max. distortion\")\n",
        "            with torch.no_grad():\n",
        "                input_features[i][0] -= input_features[i][0]\n",
        "            found += 0\n",
        "            dist.append(None)\n",
        "        if source_class[i] != true_labels[i]:\n",
        "            found += 1\n",
        "            dist.append(count[i])\n",
        "            print(\"Adversarial found changing \"+str(count[i])+\" pixels.\")\n",
        "    \n",
        "    return input_features, found, dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqsUWD3M8GYm"
      },
      "source": [
        "# Carlini Wagner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cellView": "code",
        "id": "7P1gdsc48Las"
      },
      "outputs": [],
      "source": [
        "#@title L2 attack { form-width: \"250px\" }\n",
        "\n",
        "\"\"\"The CarliniWagnerL2 attack.\"\"\"     # new only_gray_scaled \n",
        "\n",
        "INF = float(\"inf\")\n",
        "\n",
        "\n",
        "def cleverhans_carlini_wagner_l2_binary(\n",
        "    model_fn,                     # target network\n",
        "    ox,                           # original images\n",
        "    x,                            # last adversarial images that were found\n",
        "    n_classes,                    # number of classes\n",
        "    no_change,                    # search space (0 - the pixel is not allowed to be modified; 1 - pixel is allowed)\n",
        "    step=0,                       # current L0 step\n",
        "    y=None,                       # target labels\n",
        "    lr=5e-3,                      # learning rate for Adam \n",
        "    confidence=0,                 # \"kappa\" in thesis\n",
        "    clip_min=0,                   # minimum pixel value \n",
        "    clip_max=1,                   # maximum pixel value \n",
        "    initial_const=1e-2,           # initial value for constant c \n",
        "    c_max = 1e10,                  # maximum value for c \n",
        "    binary_search_steps=36,       # number of binary search steps\n",
        "    max_iterations=1000,          # maximum number of iterations for Adam\n",
        "    begin_binary_search = 5       # how often we try to double c before we start binary search\n",
        "):\n",
        "\n",
        "    def compare(pred, label, is_logits=False):\n",
        "\n",
        "        # Convert logits to predicted class if necessary\n",
        "        if is_logits:\n",
        "            pred_copy = pred.clone().detach()\n",
        "            pred_copy[label] += confidence\n",
        "            pred = torch.argmax(pred_copy)\n",
        "\n",
        "        return pred != label\n",
        "\n",
        "    if y is None: \n",
        "        # Using model predictions as ground truth to avoid label leaking\n",
        "        x = x.to(device)\n",
        "        pred = model_fn(x)\n",
        "        y = torch.argmax(pred, 1)\n",
        "\n",
        "    # Initialize some values needed for binary search on const\n",
        "    lower_bound = [0.0] * len(x)\n",
        "    upper_bound = [c_max] * len(x)\n",
        "    const = x.new_ones(len(x), 1) * initial_const\n",
        "\n",
        "    o_bestl2 = [INF] * len(x)\n",
        "    o_bestscore = [-1.0] * len(x)\n",
        "    x = torch.clamp(x, clip_min, clip_max)\n",
        "\n",
        "    o_bestattack = ox.clone().detach()\n",
        "\n",
        "    no_change= no_change.to(device)\n",
        "\n",
        "    if ox.shape[1] == 1: # gray-scaled\n",
        "        no_change_values = torch.mul(ox, torch.ones_like(no_change) - no_change)\n",
        "\n",
        "    elif ox.shape[1] == 3: # colored\n",
        "      no_change_values = torch.zeros_like(ox)\n",
        "      for i in range(ox.shape[0]):\n",
        "          for j in range(3):\n",
        "              no_change_values[i][j] += torch.mul(ox[i][j], torch.ones_like(no_change[i][0]) - no_change[i][0])\n",
        "\n",
        "    # Map images into the tanh-space\n",
        "    x = (x - clip_min) / (clip_max - clip_min)\n",
        "    x = torch.clamp(x, 0, 1)\n",
        "    x = x * 2 - 1\n",
        "    x = torch.arctanh(x * 0.999999)\n",
        "\n",
        "    done = torch.zeros_like(y) # tracks for which image we have already found an adversarial or stopped the search\n",
        "    for i in range(len(const)):\n",
        "        if const[i] >= c_max:\n",
        "            done[i] = 1\n",
        "\n",
        "    # Prepare some variables\n",
        "    modifier = torch.zeros_like(x, requires_grad=True)\n",
        "    y_onehot = torch.nn.functional.one_hot(y, n_classes).to(torch.float)    \n",
        "\n",
        "    # Define loss functions and optimizer\n",
        "    f_fn = lambda real, other: torch.max(\n",
        "        ( (real - other) )+ confidence,\n",
        "        torch.tensor(0.0).to(real.device),\n",
        "    )\n",
        "    l2dist_fn = lambda x, y: torch.pow(x - y, 2).sum(list(range(len(x.size())))[1:])\n",
        "    optimizer = torch.optim.Adam([modifier], lr=lr)\n",
        "\n",
        "    binary_search = False\n",
        "    doubled = [0 for index in range(ox.shape[0])]\n",
        "    \n",
        "    search_step = 0\n",
        "\n",
        "    # Outer loop performing binary search on const\n",
        "    while torch.min(const) <= c_max:\n",
        "        # Initialize some values needed for the inner loop\n",
        "        bestl2 = [INF] * len(x)\n",
        "        bestscore = [-1.0] * len(x)        \n",
        "        \n",
        "        # Inner loop performing attack iterations\n",
        "        for i in range(max_iterations):\n",
        "            # One attack step\n",
        "            new_x = (torch.tanh(modifier + x) + 1) / 2\n",
        "\n",
        "            new_x = torch.mul(new_x, no_change)\n",
        "            new_x = new_x + no_change_values\n",
        "\n",
        "            new_x = new_x * (clip_max - clip_min) + clip_min\n",
        "            logits = model_fn(new_x)\n",
        "\n",
        "            real = torch.sum(y_onehot * logits, 1)\n",
        "            other, _ = torch.max((1 - y_onehot) * logits - y_onehot * 1e4, 1)\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            f = f_fn(real, other) \n",
        "            l2 = l2dist_fn(new_x, ox)\n",
        "            loss = (const * f + l2).sum()\n",
        "            loss.backward(retain_graph=True)\n",
        "            optimizer.step()\n",
        "\n",
        "            if step > 0: \n",
        "                # if we are not in the initial step, save the adversarial images found so far \n",
        "                tmp = 0  # counts the number of images for which an adversarial was already found\n",
        "                predictions = torch.argmax(model(new_x),1)\n",
        "                for j in range(len(y)):\n",
        "                    if (done[j] == 0) and (y[j] != predictions[j]) and (binary_search == False): \n",
        "                        tmp += 1\n",
        "                        o_bestattack[j] = new_x[j]\n",
        "                if tmp == (len(y)-torch.sum(done)) and (binary_search == False):\n",
        "                    return o_bestattack.detach(), const          \n",
        "        \n",
        "        # Binary search step            \n",
        "        if binary_search == False:\n",
        "            for n in range(len(x)):\n",
        "                y_n = y[n]\n",
        "                bestscore[n] = torch.argmax(logits[n])\n",
        "                if compare(bestscore[n], y_n) and bestscore[n] != -1 and done[n] == 0: \n",
        "                    # Success, save adversarial\n",
        "                    o_bestattack[n] = new_x[n]\n",
        "\n",
        "                elif const[n] < c_max and done[n] == 0:\n",
        "                    # Failure, multiply by 2 if no solution found yet            \n",
        "                    const[n] *= 2\n",
        "                    doubled[n] += 1 \n",
        "\n",
        "                    if const[n] > c_max:\n",
        "                        done[n] = 1\n",
        "        \n",
        "        elif binary_search == True: # if we doubled c more than \"begin_binary_search\" times, we do binary search on c\n",
        "            if search_step == 0:\n",
        "              for n in range(len(x)):\n",
        "                y_n = y[n]\n",
        "                bestscore[n] = torch.argmax(logits[n])\n",
        "                if (compare(bestscore[n], y_n)==False):\n",
        "                    # Stop if L2 can't find an solution with c=c_max\n",
        "                    doubled[n] = 0\n",
        "                    done[n] = 1\n",
        "                    if max(doubled) == 0:\n",
        "                      # stop binary search if L2 can't find a solution for all with c=c_max\n",
        "                      binary_search = False \n",
        "\n",
        "            # binary search\n",
        "            for n in range(len(x)):\n",
        "                y_n = y[n]\n",
        "                bestscore[n] = torch.argmax(logits[n])\n",
        "                if compare(bestscore[n], y_n) and bestscore[n] != -1 and doubled[n] > begin_binary_search:\n",
        "                    # Success, save adversarial  \n",
        "                    o_bestattack[n] = new_x[n]\n",
        "                    upper_bound[n] = const[n].item() \n",
        "                    const[n] = (lower_bound[n] + upper_bound[n]) / 2\n",
        "                \n",
        "                elif search_step < binary_search_steps and doubled[n] > begin_binary_search: \n",
        "                    lower_bound[n] = const[n].item() \n",
        "                    const[n] = (lower_bound[n] + upper_bound[n]) / 2  \n",
        "\n",
        "            search_step += 1\n",
        "            \n",
        "            if search_step == binary_search_steps: \n",
        "                binary_search = False \n",
        "\n",
        "        # start binary search if doubling c didn't give a solution\n",
        "        if (binary_search == False) and (max(doubled) > begin_binary_search) and (search_step == 0) and (step > 0): \n",
        "            binary_search = True \n",
        "            for i in range(len(y)): \n",
        "                if doubled[i] > begin_binary_search: \n",
        "                      const[i] = c_max\n",
        "        \n",
        "        predictions = torch.argmax(model(o_bestattack),1)\n",
        "        for i in range(len(y)):\n",
        "            if (y[i] != predictions[i]) and (binary_search == False): \n",
        "                done[i] = 1\n",
        "   \n",
        "        if torch.sum(done) == len(y):\n",
        "            return o_bestattack.detach(), const"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "9NMw__5l8q0a"
      },
      "outputs": [],
      "source": [
        "#@title L0 attack { form-width: \"250px\" }\n",
        "\n",
        "def cw_l0_cleverhans(      \n",
        "    model_fn,                       # target network       \n",
        "    x,                              # original image\n",
        "    n_classes,                      # number of classes\n",
        "    y=None,                         # target labels\n",
        "    lr=5e-3,                        # learning rate for Adam\n",
        "    confidence=0,                   # \"kappa\" in thesis\n",
        "    clip_min=0,                     # minimum pixel value\n",
        "    clip_max=1,                     # maximum pixel value\n",
        "    initial_const=1e-4,             # initial value for constant c\n",
        "    binary_search_steps=5,          # how often we try to double c before we start binary search\n",
        "    max_iterations=1000,            # maximum number of iterations for Adam\n",
        "):\n",
        "    '''\n",
        "    outputs: \n",
        "    best_adv: adversarial images\n",
        "    '''\n",
        "\n",
        "    # Define loss functions and optimizer\n",
        "    def f(input):\n",
        "        logits = model_fn(input)\n",
        "        y_onehot = torch.nn.functional.one_hot(y, n_classes).to(torch.float)\n",
        "        real = torch.sum(y_onehot * logits, 1)\n",
        "        other, _ = torch.max((1 - y_onehot) * logits - y_onehot * 1e4, 1)\n",
        "        return (real - other) + confidence\n",
        "\n",
        "    if y is None:\n",
        "        # Using model predictions as ground truth to avoid label leaking\n",
        "        x=x.to(device)\n",
        "        pred = model_fn(x)\n",
        "        y = torch.argmax(pred, 1)\n",
        "\n",
        "    \n",
        "    no_change = torch.ones(x.shape[0], 1, x.shape[2], x.shape[3]) # in the beginning all pixels are in the search space\n",
        "\n",
        "    step = 0\n",
        "    adversarial_images = x.clone()\n",
        "    best_adv = torch.zeros_like(x)\n",
        "\n",
        "    done = torch.zeros_like(y)\n",
        "    constant_c = x.new_ones(len(x), 1)*torch.tensor(1e-4).to(device)\n",
        "    start = time.time()\n",
        "\n",
        "    while torch.sum(done) != len(y):\n",
        "\n",
        "        adversarial_images, constant_c = cleverhans_carlini_wagner_l2_binary(model, x, adversarial_images, n_classes, no_change, step, y=y, initial_const= torch.tensor(constant_c))\n",
        "        \n",
        "        # if l2 doesnt find an adversarial it returns the original image\n",
        "        predictions = torch.argmax(model(adversarial_images),1)\n",
        "        for i in range(len(predictions)):\n",
        "            if (done[i] == 0) and (predictions[i] != y[i]): # if success, save adversarial \n",
        "                best_adv[i] = adversarial_images[i]\n",
        "            elif (done[i] == 0) and (predictions[i] == y[i]): # else we are done with this image since L2 doesn't find an adversarial anymore\n",
        "                done[i] = 1\n",
        "\n",
        "        if torch.sum(done) == len(y):\n",
        "            return best_adv\n",
        "        adversarial_images.requires_grad =True \n",
        "\n",
        "        out_adv = torch.sum(f(adversarial_images))  \n",
        "        out_adv.backward()\n",
        "\n",
        "        g = adversarial_images.grad \n",
        "\n",
        "        delta = x.to(device) - adversarial_images.to(device)\n",
        "        delta = delta.to(device)\n",
        "\n",
        "        map = torch.mul(g, delta).squeeze()\n",
        "\n",
        "        if x.shape[1] == 3: # for colored images\n",
        "          map = torch.sum(map,1)\n",
        "\n",
        "        min_indices = []\n",
        "        for i in range(no_change.shape[0]):\n",
        "            #find first value where no_change != 0\n",
        "            found = 0\n",
        "            for row in range(len(no_change[i][0])):\n",
        "                for column in range(len(no_change[i][0][row])):\n",
        "                    if no_change[i][0][row][column] == 1:\n",
        "                        min_idx = row,column\n",
        "                        min_val = map[i][row][column]\n",
        "                        found = 1\n",
        "                    if found == 1:\n",
        "                        break\n",
        "                if found == 1:\n",
        "                        break\n",
        "\n",
        "            # find min pixel \n",
        "\n",
        "            for row in range(map.shape[1]):\n",
        "                for column in range(len(map[i][row])):\n",
        "                    if map[i][row][column] < min_val:\n",
        "                        if no_change[i][0][row][column] == 1:\n",
        "                            min_val = map[i][row][column]\n",
        "                            min_idx = row,column\n",
        "\n",
        "            min_indices.append(min_idx)\n",
        "\n",
        "        for i in range(no_change.shape[0]):\n",
        "            if done[i] == 0:\n",
        "                row = min_indices[i][0]\n",
        "                column = min_indices[i][1] \n",
        "                no_change[i][0][row][column] = 0\n",
        "        step += 1\n",
        "\n",
        "        adversarial_images.grad.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBR6JW8d89Ig"
      },
      "source": [
        "# ------------------------ Runner ------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1OcoxUP9Bfn"
      },
      "outputs": [],
      "source": [
        "#@title Define hyperparameters { form-width: \"250px\" }\n",
        "#@markdown Load model and dataset \n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  \n",
        "  #@markdown Dataset (f_mnist, mnist, cifar10)\n",
        "  dataset = \"cifar10\" #@param {type: \"string\"} \n",
        "  #@markdown Attack (CS, JSMA, SF, DF, CW_L0, CW_L2)\n",
        "  attack = \"CW_L0\" #@param {type: \"string\"} \n",
        "  #@markdown Batch size\n",
        "  batch_size =  2#@param {type: \"integer\"}  \n",
        "\n",
        "  parser = argparse.ArgumentParser(description='Define hyperparameters.')\n",
        "  parser.add_argument('--dataset', type=str, default=dataset, help='cifar10, mnist, f_mnist')\n",
        "  parser.add_argument('--attack', type=str, default=attack, help='CS, JSMA, SF, DF, CW_l2, CW')\n",
        "  parser.add_argument('--data_dir', type=str, default= './data')\n",
        "\n",
        "  hps = parser.parse_args([])\n",
        "\n",
        "  # load model and dataset\n",
        "\n",
        "  if hps.dataset == 'f_mnist':\n",
        "    test_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, train=False, transform=transforms.Compose([transforms.ToTensor()])) \n",
        "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "    \n",
        "    run = wandb.init(job_type=\"model-training\")\n",
        "    artifact = run.use_artifact('pretrained-model:latest')\n",
        "    artifact_dir = artifact.download()\n",
        "\n",
        "  elif hps.dataset == 'mnist':\n",
        "    transform = transforms.Compose([transforms.Resize((32, 32)),transforms.ToTensor()])\n",
        "    test_set = datasets.MNIST('DATA_MNIST/', download=True, train=False, transform=transform)\n",
        "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    run = wandb.init(project=\"SparseAdv\", job_type=\"model-training\")\n",
        "    artifact = run.use_artifact('LeNet2-modelv2:latest')\n",
        "    artifact_dir = artifact.download()\n",
        "\n",
        "  elif hps.dataset == 'cifar10':\n",
        "    test_set = torchvision.datasets.CIFAR10(root='../../data/', download=True, train=False, transform=transforms.ToTensor())\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    run = wandb.init(job_type=\"model-training\")\n",
        "    artifact = run.use_artifact('VGG_new-model2v0:latest')\n",
        "    artifact_dir = artifact.download()\n",
        "    \n",
        "  # IF YOU ARE USING GPU THEN YOU NEED TO DISABLE THIS\n",
        "  if torch.cuda.is_available():\n",
        "    model = torch.load(artifact_dir+\"/model\")\n",
        "  else:\n",
        "    model = torch.load(artifact_dir+\"/model\", map_location=torch.device('cpu'))\n",
        "  model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHFPR6aU9Qw2"
      },
      "outputs": [],
      "source": [
        "#@title Runner { form-width: \"250px\" }\n",
        "\n",
        "if hps.attack == 'CS':\n",
        "  \n",
        "  args = {'type_attack': 'L0',      \n",
        "          'n_iter': 1000,           \n",
        "          'n_max': 100,             \n",
        "          'kappa': -1,              \n",
        "          'epsilon': -1,            \n",
        "          'sparsity': 1024,         \n",
        "          'size_incr': 1}\n",
        "  \n",
        "  attack = CSattack(model, args)\n",
        "\n",
        "  batch_i = 0\n",
        "  for data in test_loader:\n",
        "      images, labels = data\n",
        "      images = torch.permute(images,(0,2,3,1)) \n",
        "      adv, pixels_changed, fl_success = attack.perturb(images.numpy(), labels.numpy())\n",
        "\n",
        "      print(\"Distortions (pixels): \"+str(pixels_changed))\n",
        "\n",
        "      batch_tensor=torch.permute(torch.tensor(adv),(0,3,1,2)).cpu()\n",
        "      grid_img = torchvision.utils.make_grid(batch_tensor, nrow=batch_size)      \n",
        "      plt.imshow(grid_img.permute(1, 2, 0))\n",
        "\n",
        "\n",
        "elif hps.attack == \"SF\":\n",
        "\n",
        "    pixel_nr_list = []\n",
        "\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        \n",
        "        adv_batch = torch.zeros_like(images)\n",
        "\n",
        "        pixel_nr_list_batch = []\n",
        "        for i in range(len(labels)):\n",
        "          x_0 = images[i].to(device)\n",
        "          fool_im, r, pred_label, fool_label, loops = sparsefool(x_0[None,:,:,:], model, lb=0, ub=1, lambda_=1., max_iter=20, epsilon=0.02, device='cuda')  # shape of fool_im [1,1,32,32]\n",
        "          \n",
        "          # compute distortion\n",
        "          if r.shape[1] == 1:\n",
        "            pert = torch.count_nonzero(r).item()\n",
        "            pixel_nr_list.append(pert)\n",
        "            pixel_nr_list_batch.append(pert)\n",
        "            adv_batch[i] = fool_im.squeeze(0)\n",
        "          elif r.shape[1] == 3: \n",
        "            a2 = torch.sum(r,1)\n",
        "            pert = torch.count_nonzero(a2).item()\n",
        "            pixel_nr_list.append(pert)\n",
        "            pixel_nr_list_batch.append(pert)\n",
        "            adv_batch[i] = fool_im.squeeze(0)\n",
        "\n",
        "        predictions = torch.argmax(model(adv_batch.to(device)),1)\n",
        "        for i in range(20):\n",
        "          if predictions[i] == labels[i]:\n",
        "            print(\"Failed to find adversarial for image \"+str(i))\n",
        "\n",
        "        print(\"Distortions (pixels): \"+str(pixel_nr_list_batch))\n",
        "\n",
        "        batch_tensor=adv_batch\n",
        "        grid_img = torchvision.utils.make_grid(batch_tensor, nrow=batch_size)\n",
        "        plt.imshow(grid_img.permute(1,2,0))\n",
        "\n",
        "\n",
        "elif hps.attack == 'JSMA':\n",
        "\n",
        "  distortion_list = []\n",
        "  \n",
        "  for data in test_loader:\n",
        "\n",
        "      images, labels = data\n",
        "      \n",
        "      target_tensor = torch.randint(0,10,[batch_size])\n",
        "\n",
        "      # compute randon target classes\n",
        "      for i in range(target_tensor.shape[0]):\n",
        "          while target_tensor[i] == labels[i]:\n",
        "                target_tensor[i] = random.randint(0, 9)\n",
        "\n",
        "      adv, number, dist = jsma_batch(model, images.to(device), labels, target_tensor) \n",
        "      \n",
        "      print(\"Distortions (pixels): \"+str(dist))\n",
        "\n",
        "      batch_tensor=adv.cpu() \n",
        "      grid_img = torchvision.utils.make_grid(batch_tensor, nrow=batch_size) \n",
        "      plt.imshow(grid_img.permute(1, 2, 0))\n",
        "\n",
        "\n",
        "elif hps.attack == \"CW_L2\":    \n",
        "  for data in test_loader:\n",
        "      images, labels = data\n",
        "      # gray scaled, no restriction on search space\n",
        "      no_change = torch.ones_like(images)   \n",
        "\n",
        "      # colored, no restriction on search space\n",
        "      #no_change = torch.ones(images.shape[0],1,images.shape[2], images.shape[3]) \n",
        "\n",
        "      # gray scaled, restriction to a random subset of pixels \n",
        "      #no_change = torch.randint(0,2,images.shape)  \n",
        "\n",
        "      # colored, restriction to a random subset of pixels \n",
        "      #no_change = torch.randint(0,2,(images.shape[0],1,images.shape[2], images.shape[3]))        \n",
        "       \n",
        "      no_change = no_change.to(device)\n",
        "\n",
        "      images = images.to(device)   \n",
        "      out_img,_ = cleverhans_carlini_wagner_l2_binary(model, images, images, 10, no_change ) \n",
        "\n",
        "      # calculate distortion\n",
        "      Dist = []\n",
        "      for image in range(images.shape[0]):\n",
        "          tmp = 0\n",
        "          for color in range (images.shape[1]):\n",
        "              for i in range(images.shape[2]):                \n",
        "                  for j in range(images.shape[3]):\n",
        "                      if out_img[image][color][i][j] != images[image][color][i][j]:\n",
        "                          tmp += 1\n",
        "              Dist.append(tmp)\n",
        "  \n",
        "      images = images.cpu()\n",
        "\n",
        "      predictions = torch.argmax(model(out_img),1)\n",
        "      print(\"pred\")\n",
        "      print(predictions)\n",
        "      print(\"labels\")\n",
        "      print(labels)\n",
        "\n",
        "      batch_tensor=out_img.cpu()\n",
        "      grid_img = torchvision.utils.make_grid(batch_tensor, nrow=batch_size)\n",
        "      plt.imshow(grid_img.permute(1, 2, 0))\n",
        "\n",
        "\n",
        "elif hps.attack == \"CW_L0\": \n",
        "  pixel_nr_list_batch = []\n",
        "  for data in test_loader:\n",
        "      images, labels = data\n",
        "      no_change = torch.ones_like(images)\n",
        "      images = images.to(device)\n",
        "      out_img = cw_l0_cleverhans(model_fn = model, x = images, n_classes=10)\n",
        "    \n",
        "      # calculate distortion\n",
        "      for img in range(images.shape[0]):\n",
        "          r = out_img[img].cpu() - images[img].cpu() \n",
        "          if r.shape[0] == 1:\n",
        "              pert = torch.count_nonzero(r).item()\n",
        "              pixel_nr_list_batch.append(pert)\n",
        "          elif r.shape[0] == 3: \n",
        "              a2 = torch.sum(r,0)\n",
        "              pert = torch.count_nonzero(a2).item()\n",
        "              pixel_nr_list_batch.append(pert)\n",
        "\n",
        "      print(\"Distortions (pixels): \" + str(pixel_nr_list_batch)) \n",
        "      \n",
        "      batch_tensor=out_img.cpu()\n",
        "      grid_img = torchvision.utils.make_grid(batch_tensor, nrow=batch_size)\n",
        "      plt.imshow(grid_img.permute(1, 2, 0))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "SparseAdversarialAttacks.ipynb",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}